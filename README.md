# Attention_Transformer_TimeSeries

A sequence-to-sequence transformer model based on the “Attention is All You Need” 
architecture. The model will process an input sequence of 6-dimensional vectors with a length of 
100 using an encoder to create context. The decoder will start with a token of dimension 1 and 
sequence length 10, then generate the remaining 90 outputs in a sequence-to-sequence manner.

MAE Score: 0.0831
